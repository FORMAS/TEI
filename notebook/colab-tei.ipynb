{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TEI.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz6zSdnaUF6D"
      },
      "source": [
        " TEI - TimeBankPT Event Identification\n",
        "\n",
        "\n",
        " [![Github](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/FORMAS/TEFE)\n",
        "\n",
        "[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)](https://hub.docker.com/r/andersonsacramento/tefe)\n",
        "\n",
        "\n",
        "# DESCRIPTION\n",
        "TEI is an event trigger identifier system for sentences in the Portuguese language. It locates the event trigger terms in a sentence. The model was trained on the TimeBankPT (COSTA; BRANCO,2012) corpus. \n",
        "\n",
        "\n",
        "## How to cite this work\n",
        "\n",
        "Peer-reviewed accepted paper:\n",
        "\n",
        "10th Brazilian Conference on Intelligent Systems (BRACIS)\n",
        "\n",
        "* Sacramento A. ; Souza M. . Joint Event Extraction with Contextualized Word Embeddings for the Portuguese \n",
        "Language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKsil3MZWxtw"
      },
      "source": [
        "# Download and locate BERTimbau Base model and TEI model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwUnMDQlUCTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb0360d5-daed-4c93-edfc-ddda59643d73"
      },
      "source": [
        "!pip install gdown"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.62.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pijxkLn5U_BT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b5cf179-0db0-4968-9796-3bf942fd6a36"
      },
      "source": [
        "!gdown --id 1tQ8ayODyz1VDI3SGTcSYLO6CmaHRCsoW --output tei.zip\n",
        "!unzip tei.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tQ8ayODyz1VDI3SGTcSYLO6CmaHRCsoW\n",
            "To: /content/tei.zip\n",
            "2.58MB [00:00, 9.71MB/s]\n",
            "Archive:  tei.zip\n",
            "   creating: models/\n",
            "  inflating: models/tiff.h5          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCehgYCFVFiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2389a88-dbbc-47ea-d463-fbc4f78e7f10"
      },
      "source": [
        "!gdown --id 1qIR2GKpBqB-sOmX0Q5j1EQ6NSugYMCsX --output bertimbau.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qIR2GKpBqB-sOmX0Q5j1EQ6NSugYMCsX\n",
            "To: /content/bertimbau.zip\n",
            "1.21GB [00:12, 93.1MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcPQ1rurVQgX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45acfc0a-03f3-40cd-85e1-531be12cbb91"
      },
      "source": [
        "!mv bertimbau.zip models/\n",
        "!unzip models/bertimbau.zip -d models/\n",
        "!rm models/bertimbau.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  models/bertimbau.zip\n",
            "  inflating: models/BERTimbau/bert_model.ckpt.index  \n",
            "  inflating: models/BERTimbau/bert_config.json  \n",
            "  inflating: models/BERTimbau/vocab.txt  \n",
            "  inflating: models/BERTimbau/bert_model.ckpt.meta  \n",
            "  inflating: models/BERTimbau/bert_model.ckpt.data-00000-of-00001  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GekDCRbVTeQ"
      },
      "source": [
        "# Load TEI code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfqvemR8VYn9"
      },
      "source": [
        "## install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzqVs2joVRPl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b604f222-2056-49cb-974b-d4ee485862ff"
      },
      "source": [
        "!pip install tensorflow>=2.6.0\n",
        "!pip install keras-bert>=0.88\n",
        "!pip install numpy"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxg9wh4yVeDF"
      },
      "source": [
        "## load functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-TM5QQHVbwy"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import glob\n",
        "\n",
        "from keras_bert import load_vocabulary, load_trained_model_from_checkpoint, Tokenizer, get_checkpoint_paths\n",
        "from keras_bert.datasets import get_pretrained, PretrainedList\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "BERTIMBAU_MODEL_PATH = 'models/BERTimbau/'\n",
        "EMBEDDING_ID = 'sum_all_12'\n",
        "\n",
        "\n",
        "\n",
        "def tokenize_and_compose(text):\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        text_tokens = []\n",
        "        for i, token in enumerate(tokens):\n",
        "            split_token = token.split(\"##\")\n",
        "            if len(split_token) > 1:\n",
        "                token = split_token[1]\n",
        "                text_tokens[-1] += token\n",
        "            else:\n",
        "                text_tokens.append(token)\n",
        "        if len(text_tokens[1:-1]) == 1:\n",
        "          return text_tokens[1]\n",
        "        else:\n",
        "          return text_tokens[1:-1]\n",
        "\n",
        "\n",
        "def compose_token_embeddings(sentence, tokenized_text, embeddings):\n",
        "        tokens_indices_composed = [0] * len(tokenized_text)\n",
        "        j = -1\n",
        "        for i, x in enumerate(tokenized_text):\n",
        "            if x.find('##') == -1:\n",
        "                j += 1\n",
        "            tokens_indices_composed[i] = j\n",
        "        word_embeddings = [0] * len(set(tokens_indices_composed))\n",
        "        j = 0\n",
        "        for i, embedding in enumerate(embeddings):\n",
        "            if j == tokens_indices_composed[i]:\n",
        "                word_embeddings[j] = embedding\n",
        "                j += 1\n",
        "            else:\n",
        "                word_embeddings[j - 1] += embedding\n",
        "        return word_embeddings\n",
        "\n",
        "def extract(text, options={'sum_all_12':True}, seq_len=512, output_layer_num=12):\n",
        "        features = {k:v for (k,v) in options.items() if v}\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        indices, segments = tokenizer.encode(first = text, max_len = seq_len)\n",
        "        predicts = model_bert.predict([np.array([indices]), np.array([segments])])[0]\n",
        "        predicts = predicts[1:len(tokens)-1,:].reshape((len(tokens)-2, output_layer_num, 768))\n",
        "\n",
        "        for (k,v) in features.items():\n",
        "            if k == 'sum_all_12':\n",
        "                features[k] = compose_token_embeddings(text, tokens[1:-1], predicts.sum(axis=1))\n",
        "            if k == 'sum_last_4':\n",
        "                features[k] = compose_token_embeddings(text, tokens[1:-1], predicts[:,-4:,:].sum(axis=1))\n",
        "            if k == 'concat_last_4':\n",
        "                features[k] = compose_token_embeddings(text, tokens[1:-1], predicts[:,-4:,:].reshape((len(tokens)-2,768*4)))\n",
        "            if k == 'last_hidden':\n",
        "                features[k] = compose_token_embeddings(text, tokens[1:-1], predicts[:,-1:,:].reshape((len(tokens)-2, 768)))\n",
        "        return features\n",
        "\n",
        "\n",
        "\n",
        "def get_sentence_original_tokens(sentence, tokens):\n",
        "        token_index = 0\n",
        "        started = False\n",
        "        sentence_pos_tokens = []\n",
        "        i = 0\n",
        "        while i < len(sentence):\n",
        "                if sentence[i] != ' ' and not started:\n",
        "                        start = i\n",
        "                        started = True\n",
        "                if sentence[i] == tokens[token_index] and started:\n",
        "                        sentence_pos_tokens.append(sentence[i])\n",
        "                        started = False\n",
        "                        token_index += 1\n",
        "                elif i<len(sentence) and (sentence[i] == ' ' or tokenize_and_compose(sentence[start:i+1]) == tokens[token_index] ) and started:\n",
        "                        sentence_pos_tokens.append(sentence[start:i+1])\n",
        "                        start = i+1\n",
        "                        started = False\n",
        "                        token_index += 1\n",
        "                i += 1\n",
        "        return sentence_pos_tokens\n",
        "\n",
        "\n",
        "def get_text_location(text, arg, start_search_at=0):\n",
        "    text = text.lower()\n",
        "    arg = arg.lower()\n",
        "    pattern = re.compile(r'\\b%s\\b' % arg)\n",
        "    match = pattern.search(text, start_search_at)\n",
        "    if match:\n",
        "        return (match.start(), match.end())\n",
        "    else:\n",
        "        return (-1, -1)\n",
        "\n",
        "\n",
        "def predict_events(text, feature_option):\n",
        "    text_tokens = get_sentence_original_tokens(text, tokenize_and_compose(text))\n",
        "    features = extract(text, {feature_option:True})[feature_option]\n",
        "    embedding = np.array(features).reshape((len(text_tokens), 768))\n",
        "    prediction = [model.predict(e.reshape((1, 768))) for e in embedding ]\n",
        "    positions = list(filter((lambda i: i>= 0 and i < len(text_tokens)), [pos if pred_value > 0.5 else -1 for (pos, pred_value) in enumerate(prediction)]))\n",
        "    output = []\n",
        "    if len(positions) > 0:\n",
        "        start_at = sum([len(token) for token in text_tokens[:positions[0]]])\n",
        "    for pos in positions:\n",
        "        loc_start, loc_end = get_text_location(text, text_tokens[pos], start_at)\n",
        "        start_at = loc_end\n",
        "        output.append({'text':  text[loc_start:loc_end],\n",
        "                       'start': loc_start,\n",
        "                       'end':   loc_end})\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_bertimbau_model():    \n",
        "    global tokenizer\n",
        "    global model_bert\n",
        "        \n",
        "    paths = get_checkpoint_paths(BERTIMBAU_MODEL_PATH)\n",
        "\n",
        "    model_bert = load_trained_model_from_checkpoint(paths.config, paths.checkpoint, seq_len=512, output_layer_num=12)\n",
        "\n",
        "    token_dict = load_vocabulary(paths.vocab)\n",
        "    tokenizer = Tokenizer(token_dict)\n",
        "\n",
        "\n",
        "def load_tei_model():\n",
        "    global model\n",
        "\n",
        "    model = load_model('models/tiff.h5')\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def identify_from_files(input_path, output_path):\n",
        "    for filepathname in glob.glob(f'{input_path}*.txt'):\n",
        "        extractions = []\n",
        "        for line in open(filepathname):\n",
        "            line = line.strip()\n",
        "            print(line)\n",
        "            extractions.append(predict_events(line, EMBEDDING_ID))\n",
        "        filename = filepathname.split('.txt')[0].split(os.sep)[-1]\n",
        "        with open(f'{output_path}{filename}.json', 'w')  as outfile:\n",
        "            json.dump(extractions, outfile)\n",
        "        print(f'{filename}')\n",
        "\n",
        "\n",
        "def identify_events_from(input_path, output_path):\n",
        "    run_identification_context(lambda : identify_from_files(input_path, output_path))\n",
        "        \n",
        "\n",
        "def identify_events_from_sentence(sentence):\n",
        "    sentence = sentence.strip()\n",
        "    run_identification_context(lambda : print(predict_events(sentence, EMBEDDING_ID)))\n",
        "        \n",
        "\n",
        "def run_identification_context(run_identification_func):                        \n",
        "    if len(tf.config.list_physical_devices('GPU')) > 0:\n",
        "        with tf.device('/GPU:0'):\n",
        "            load_bertimbau_model()\n",
        "            load_tei_model()\n",
        "            run_identification_func()\n",
        "    else:\n",
        "        with tf.device(\"/device:CPU:0\"):\n",
        "            load_bertimbau_model()\n",
        "            load_tei_model()\n",
        "            run_identification_func()\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN8h5VRJVxD7"
      },
      "source": [
        "# RUN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzupmmOiVz2Z"
      },
      "source": [
        "# Identify Events From Sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GekH9kzVyJ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb1eb51-4a21-4f7f-f24e-efc641d99b9b"
      },
      "source": [
        "#@title Input the sentence\n",
        "\n",
        "sentence = 'Vazamentos de dados exp\\xF5em senhas de funcion\\xE1rios do governo, diz relat\\xF3rio.' #@param {type:\"string\"}\n",
        "\n",
        "print(sentence)\n",
        "identify_events_from_sentence(sentence)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vazamentos de dados expõem senhas de funcionários do governo, diz relatório.\n",
            "[{'text': 'Vazamentos', 'start': 0, 'end': 10}, {'text': 'expõem', 'start': 20, 'end': 26}, {'text': 'diz', 'start': 62, 'end': 65}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZCuBBJFWT_D"
      },
      "source": [
        "## Identify Events From Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFmxAawiWSRb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5519c8ec-74b6-4540-dfa9-6a805d630a82"
      },
      "source": [
        "# If you want to be able to process files from your drive folders \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-QR1vdIWXXT"
      },
      "source": [
        "#@title ## Input and Output directory fields\n",
        "\n",
        "#@markdown The text files in the input directory are expected to have the format:\n",
        "\n",
        "#@markdown * all text files end with the extension .txt\n",
        "#@markdown * sentences are separated by newlines\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Enter the directories paths:\n",
        "input_dir = \"/content/drive/MyDrive/input-files/\" #@param {type:\"string\"}\n",
        "output_dir = \"/content/drive/MyDrive/output-files/\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "identify_events_from(input_dir, output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}